{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import pyodbc\n",
    "import re\n",
    "from datetime import datetime\n",
    "# URL base de la pag\n",
    "base_url = \"https://www.freyafitness.com.ar/productos/?mpage=3\"\n",
    "\n",
    "\n",
    "# lista para almacenar los productos y precios\n",
    "productos_totales = []\n",
    "precios_totales = []\n",
    "link_producto=[]\n",
    "\n",
    "# lista de palabras clave de marca\n",
    "palabras_clave = {\n",
    "    1: ['Agebiologique', 'Age Biologique', 'Age'],\n",
    "    2: ['Athom X'],\n",
    "    3: ['Growth Supplements', '+Growth'],\n",
    "    4: ['HTN'],\n",
    "    5: ['HochSport'],\n",
    "    6: ['Ena', 'ENA SPORT'],\n",
    "    7: ['Star Nutrition'],\n",
    "    8: ['Gentech'],\n",
    "    9: ['Nutremax'],\n",
    "    10: ['Body Advance', 'BODY ADVANCE', 'BODY'],\n",
    "    11: ['Nutrilab'],\n",
    "    12: ['Hardcore Nutrition', 'Hard Core', 'Hard'],\n",
    "    13: ['Universal Nutrition'],\n",
    "    14: ['Gold Nutrition'],\n",
    "    15: ['Mervicklab'],\n",
    "    16: ['Ultra tech', 'Ultra Tech', 'Ultratech Nutrition'],\n",
    "    17: ['Vitatech'],\n",
    "    18: ['Protein Factory'],\n",
    "    19: ['Protein Project'],\n",
    "    20: ['Xtrenght', 'xtrength ', 'Xtrenght Nutrition'],\n",
    "    21: ['Xing Yu'],\n",
    "    22: ['Erox'],\n",
    "    23: ['HARDY'],\n",
    "    24: ['Xxl'],\n",
    "    25: ['SPX'],\n",
    "    26: ['Desconocido']\n",
    "}\n",
    "\n",
    "\n",
    "# diccionario de categorias y palabras clave\n",
    "tipos_de_productos = {\n",
    "    1: ['creatina', 'monohidrato'],\n",
    "    2: ['proteina', 'Proteínas', 'whey', 'protein', 'Whey'],\n",
    "    3: ['vitamina', 'multivitaminico'],\n",
    "    4: ['Collagen', 'Colageno'],\n",
    "    5: ['Gluta', 'l-glutamina', 'Glutamina'],\n",
    "    6: ['Quemador De Grasa', 'Carntina', 'Quemador De Grasas'],\n",
    "    7: ['Aminoácidos', 'Aminoacidos', 'bcaa'],\n",
    "    8: ['Cafeína', 'Cafeina'],\n",
    "    9: ['Zma', 'zma', 'magnesio'],\n",
    "    10: ['Pre Work', 'Pre entreno', 'prework', 'preentreno'],\n",
    "    11: ['Ganador De Peso', 'Ganador', 'ganador', 'Ultra Mass', 'Mutant Mass'],\n",
    "    12: ['Bebida Isotónica', 'bebida', 'Bebida'],\n",
    "    13: ['Gel', 'Energy', 'energy', 'Enargy', 'Race'],\n",
    "    14: ['Hmb', 'hmb'],\n",
    "    15: ['Vigorizante', 'vigorizante', 'Potenciador', 'potenciador', 'Testo', 'testo'],\n",
    "    16:['Desconocido']\n",
    "}\n",
    "\n",
    "\n",
    "# obtener la fecha y hora de scraping\n",
    "fecha_scraping = datetime.now()\n",
    "\n",
    "# recorrer las paginas \n",
    "for pagina in range(1, 27):  \n",
    "    # URL de la pagina actual\n",
    "    url = base_url + f\"?page={pagina}\"\n",
    "    \n",
    "    # obtener el HTML de la página actual\n",
    "    url_obtenido = requests.get(url)\n",
    "    html_obtenido = url_obtenido.text\n",
    "    \n",
    "    # parsear el contenido\n",
    "    soup = BeautifulSoup(html_obtenido, \"html.parser\")\n",
    "    \n",
    "    # encontrar los productos y precios de la pag actual\n",
    "    separadores = soup.find_all('a', class_=\"js-item-name item-name\")\n",
    "    price = soup.find_all('span', class_=\"js-price-display item-price h6\")\n",
    "    \n",
    "    # agregar los productos y precios a las listas totales\n",
    "    for producto, precio in zip(separadores, price):\n",
    "        productos_totales.append(producto.get('aria-label'))\n",
    "        precios_totales.append(precio.text.strip())\n",
    "\n",
    "      #encontrar los links del producto\n",
    "    links = soup.find_all('a', class_=\"js-item-name item-name\")\n",
    "    for i in links: \n",
    "        link_producto.append(i.get('href'))\n",
    "df = pd.DataFrame({'Producto': productos_totales, 'Precio': precios_totales, 'Link_Producto': link_producto})\n",
    "\n",
    "def encontrar_marca(row):\n",
    "    producto = row['Producto']\n",
    "    for id_marca, palabras in palabras_clave.items():\n",
    "        for palabra in palabras:\n",
    "            if palabra.lower() in producto.lower():\n",
    "                return id_marca\n",
    "    return 26  \n",
    "\n",
    "# funcion para determinar el tipo de producto\n",
    "def clasificar_tipo_producto(row):\n",
    "    producto = row['Producto']  \n",
    "    for tipo, palabras_clave in tipos_de_productos.items():\n",
    "        for palabra in palabras_clave:\n",
    "            if re.search(r'\\b' + palabra + r'\\b', producto, re.IGNORECASE):\n",
    "                return tipo\n",
    "    return 16\n",
    "\n",
    "# agregar la columna \"fecha_de_scraping\"\n",
    "df['fecha_scraping'] = fecha_scraping\n",
    "\n",
    "# agregar la columna para el tipo de producto\n",
    "df['id_tipo'] = df.apply(clasificar_tipo_producto, axis=1)\n",
    "\n",
    "\n",
    "# aplicar la funcion a cada fila y crear la columna \"marca\"\n",
    "df['id_marca'] = df.apply(encontrar_marca, axis=1)\n",
    "# Reorganizar el orden de las columnas en el DataFrame\n",
    "df = df[['Producto', 'id_marca', 'id_tipo', 'Precio', 'fecha_scraping', 'Link_Producto']]\n",
    "\n",
    "\n",
    "# configurar la conexión a SQL Server\n",
    "server = ''\n",
    "database = ''\n",
    "username = ''\n",
    "password = ''\n",
    "driver = '{ODBC Driver 17 for SQL Server}'  \n",
    "\n",
    "# crear la cadena de conexion\n",
    "conn_str = f'DRIVER={driver};SERVER={server};DATABASE={database};UID={username};PWD={password};'\n",
    "\n",
    "#conector \n",
    "conn = pyodbc.connect(conn_str)\n",
    "\n",
    "# nombre de la tabla en SQL Server\n",
    "tabla_sql = 'offsuple'\n",
    "\n",
    "# convertir el df en una lista de tuplas\n",
    "records = [tuple(row) for row in df.values]\n",
    "\n",
    "# query SQL para insertar datos en la tabla\n",
    "query = f\"INSERT INTO {tabla_sql} ({', '.join(df.columns)}) VALUES ({', '.join(['?'] * len(df.columns))})\"\n",
    "\n",
    "# ejecutar la query para insertar los datos\n",
    "cursor = conn.cursor()\n",
    "cursor.executemany(query, records)\n",
    "conn.commit()\n",
    "\n",
    "# cerrar la conexion\n",
    "conn.close()\n",
    "\n",
    "# opciones csv y excel\n",
    "# df.to_csv('Lista_de_precios_TMS.csv', index=False)\n",
    "# df.to_excel('Lista_de_precios_TMS.xlsx', index=False)\n",
    "\n",
    "print(\"Datos scrapeados y cargados exitosamente en SQL Server.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
